{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (2.2.1)\n",
      "Requirement already satisfied: numpy<2,>=1.23.2 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from pandas) (2.9.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_30336\\1728903199.py:97: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  top_recommendations = merged_pps_xy.groupby('name').apply(get_top_recommendations).reset_index(drop=True)\n",
      "C:\\Users\\juani\\AppData\\Local\\Temp\\ipykernel_30336\\1728903199.py:104: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  final_recommendations = top_recommendations.groupby('name').apply(format_recommendations).reset_index()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>player</th>\n",
       "      <th>recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Carlos Boozer</td>\n",
       "      <td>Restricted Area: Maintain Frequency; Left Pain...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Dwight Buycks</td>\n",
       "      <td>Left Corner 3: Shoot More; Left Paint (Non-RA)...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Ed Davis</td>\n",
       "      <td>Restricted Area: Maintain Frequency; Right Pai...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Jabari Brown</td>\n",
       "      <td>Right Corner 3: Shoot More; Left Above the Bre...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Jeremy Lin</td>\n",
       "      <td>Right Corner 3: Shoot More; Restricted Area: S...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          player                                    recommendations\n",
       "0  Carlos Boozer  Restricted Area: Maintain Frequency; Left Pain...\n",
       "1  Dwight Buycks  Left Corner 3: Shoot More; Left Paint (Non-RA)...\n",
       "2       Ed Davis  Restricted Area: Maintain Frequency; Right Pai...\n",
       "3   Jabari Brown  Right Corner 3: Shoot More; Left Above the Bre...\n",
       "4     Jeremy Lin  Right Corner 3: Shoot More; Restricted Area: S..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "file_path = 'mergeFinalC_with_local.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# Define PPS calculation function\n",
    "def calculate_pps(row):\n",
    "    if row['shot_made_flag'] == 1:\n",
    "        if row['shot_distance'] > 23.75:  # Assuming 3-point shots are beyond 23.75 feet\n",
    "            return 3\n",
    "        else:\n",
    "            return 2\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "# Apply PPS calculation\n",
    "data['pps'] = data.apply(calculate_pps, axis=1)\n",
    "\n",
    "# Define zones based on shot_distance and x, y coordinates\n",
    "def categorize_zone_xy(row):\n",
    "    if row['shot_distance'] <= 5:\n",
    "        return 'Restricted Area'\n",
    "    elif row['shot_distance'] <= 15:\n",
    "        if row['x'] > 0:\n",
    "            return 'Right Paint (Non-RA)'\n",
    "        else:\n",
    "            return 'Left Paint (Non-RA)'\n",
    "    elif row['shot_distance'] <= 22:\n",
    "        if row['x'] > 0:\n",
    "            return 'Right Mid-Range'\n",
    "        else:\n",
    "            return 'Left Mid-Range'\n",
    "    elif row['shot_distance'] > 22:\n",
    "        if abs(row['x']) > 22:\n",
    "            if row['x'] > 0:\n",
    "                return 'Right Corner 3'\n",
    "            else:\n",
    "                return 'Left Corner 3'\n",
    "        else:\n",
    "            if row['x'] > 0:\n",
    "                return 'Right Above the Break 3'\n",
    "            else:\n",
    "                return 'Left Above the Break 3'\n",
    "    else:\n",
    "        return 'Other'\n",
    "\n",
    "data['shot_zone_xy'] = data.apply(categorize_zone_xy, axis=1)\n",
    "\n",
    "# Group by player and custom shot zone with x, y coordinates\n",
    "grouped_xy = data.groupby(['name', 'shot_zone_xy'])\n",
    "\n",
    "# Calculate player PPS for each zone\n",
    "player_pps_xy = grouped_xy['pps'].mean().reset_index()\n",
    "\n",
    "# Calculate league average PPS for each zone\n",
    "league_grouped_xy = data.groupby(['shot_zone_xy'])\n",
    "league_pps_xy = league_grouped_xy['pps'].mean().reset_index()\n",
    "league_pps_xy.rename(columns={'pps': 'league_pps'}, inplace=True)\n",
    "\n",
    "# Merge player PPS with league PPS\n",
    "merged_pps_xy = pd.merge(player_pps_xy, league_pps_xy, on='shot_zone_xy')\n",
    "\n",
    "# Calculate shot frequency for each player in each zone\n",
    "player_shot_counts_xy = grouped_xy['pps'].count().reset_index()\n",
    "player_shot_counts_xy.rename(columns={'pps': 'shot_count'}, inplace=True)\n",
    "\n",
    "# Calculate total shots for each player\n",
    "total_shots_xy = data.groupby('name')['pps'].count().reset_index()\n",
    "total_shots_xy.rename(columns={'pps': 'total_shots'}, inplace=True)\n",
    "\n",
    "# Merge shot counts with total shots\n",
    "player_shot_counts_xy = pd.merge(player_shot_counts_xy, total_shots_xy, on='name')\n",
    "player_shot_counts_xy['shot_frequency'] = player_shot_counts_xy['shot_count'] / player_shot_counts_xy['total_shots']\n",
    "\n",
    "# Merge shot frequency data with PPS data\n",
    "merged_pps_xy = pd.merge(merged_pps_xy, player_shot_counts_xy[['name', 'shot_zone_xy', 'shot_frequency']], on=['name', 'shot_zone_xy'])\n",
    "\n",
    "# Generate recommendations\n",
    "def generate_recommendation_xy(row):\n",
    "    if row['pps'] > row['league_pps']:\n",
    "        if row['shot_frequency'] < 0.2:  # Arbitrary threshold to avoid recommending high-frequency zones\n",
    "            return 'Shoot More'\n",
    "        else:\n",
    "            return 'Maintain Frequency'\n",
    "    else:\n",
    "        return 'Shoot Less'\n",
    "\n",
    "merged_pps_xy['recommendation'] = merged_pps_xy.apply(generate_recommendation_xy, axis=1)\n",
    "\n",
    "# Filter to keep only the strongest recommendations (minimum 2, maximum 4 per player)\n",
    "def get_top_recommendations(df):\n",
    "    df = df.sort_values(by=['pps'], ascending=False)\n",
    "    top_recommendations = df.head(4) if len(df) > 4 else df\n",
    "    return top_recommendations\n",
    "\n",
    "top_recommendations = merged_pps_xy.groupby('name').apply(get_top_recommendations).reset_index(drop=True)\n",
    "\n",
    "# Format the recommendations into a single string per player\n",
    "def format_recommendations(df):\n",
    "    recommendations = df['shot_zone_xy'] + \": \" + df['recommendation']\n",
    "    return \"; \".join(recommendations)\n",
    "\n",
    "final_recommendations = top_recommendations.groupby('name').apply(format_recommendations).reset_index()\n",
    "final_recommendations.columns = ['player', 'recommendations']\n",
    "\n",
    "# Save the final recommendations to a CSV file\n",
    "final_recommendations.to_csv('Player_Recommendations.csv', index=False)\n",
    "\n",
    "# Display the final recommendations\n",
    "final_recommendations.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from matplotlib.patches import Circle, Rectangle, Arc\n",
    "\n",
    "def draw_lakers_court(ax=None, lw=2, outer_lines=False):\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "\n",
    "    violet = '#552583'\n",
    "    yellow = '#FDB927'\n",
    "\n",
    "    hoop = Circle((0, 0), radius=7.5, linewidth=lw, color=violet, fill=False)\n",
    "    backboard = Rectangle((-30, -7.5), 60, -1, linewidth=lw, color=violet)\n",
    "    outer_box = Rectangle((-80, -47.5), 160, 190, linewidth=lw, color=violet, fill=False)\n",
    "    inner_box = Rectangle((-60, -47.5), 120, 190, linewidth=lw, color=violet, fill=False)\n",
    "    top_free_throw = Arc((0, 142.5), 120, 120, theta1=0, theta2=180, linewidth=lw, color=violet)\n",
    "    bottom_free_throw = Arc((0, 142.5), 120, 120, theta1=180, theta2=0, linewidth=lw, color=violet, linestyle='dashed')\n",
    "    restricted = Circle((0, 0), radius=30, linewidth=lw, color=violet, linestyle='dashed', fill=False)\n",
    "    corner_three_a = Rectangle((-220, -47.5), 0, 140, linewidth=lw, color=violet)\n",
    "    corner_three_b = Rectangle((220, -47.5), 0, 140, linewidth=lw, color=violet)\n",
    "    three_arc = Arc((0, 0), 475, 475, theta1=22, theta2=158, linewidth=lw, color=violet)\n",
    "    center_outer_arc = Arc((0, 422.5), 120, 120, theta1=180, theta2=0, linewidth=lw, color=yellow)\n",
    "    center_inner_arc = Arc((0, 422.5), 40, 40, theta1=180, theta2=0, linewidth=lw, color=yellow)\n",
    "    \n",
    "    court_elements = [hoop, backboard, outer_box, inner_box, top_free_throw, bottom_free_throw,\n",
    "                      restricted, corner_three_a, corner_three_b, three_arc, center_outer_arc, center_inner_arc]\n",
    "    for element in court_elements:\n",
    "        ax.add_patch(element)\n",
    "\n",
    "    if outer_lines:\n",
    "        outer_lines = Rectangle((-250, -47.5), 500, 470, linewidth=lw, color=violet, fill=False)\n",
    "        ax.add_patch(outer_lines)\n",
    "\n",
    "    ax.set_xlim(-250, 250)\n",
    "    ax.set_ylim(-47.5, 422.5)\n",
    "    ax.set_aspect('equal')\n",
    "    return ax\n",
    "\n",
    "# Load data\n",
    "data_path = 'mergeFinalC_with_local.csv'\n",
    "shots_data = pd.read_csv(data_path)\n",
    "\n",
    "# Get unique player names\n",
    "players = shots_data['name'].unique()\n",
    "\n",
    "# Create a shot chart for each player\n",
    "for player in players:\n",
    "    player_shots = shots_data[shots_data['name'] == player]\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 13))\n",
    "    draw_lakers_court(ax, outer_lines=True)\n",
    "    \n",
    "    sns.scatterplot(data=player_shots, x='x', y='y', hue='shot_made_flag', palette=['red', 'green'], style='shot_made_flag',\n",
    "                    markers=['o', 'o'], s=50, alpha=0.6)\n",
    "    \n",
    "    plt.xlim(-250, 250)\n",
    "    plt.ylim(-47.5, 422.5)\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.title(f'{player} Lakers Shot Chart', fontsize=16)\n",
    "    plt.legend(title='Shot Result', loc='upper right', fontsize=12, title_fontsize=12)\n",
    "    \n",
    "    # Save the figure\n",
    "    plt.savefig(f'{player}_shot_chart.png')\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting webdriver-manager\n",
      "  Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from webdriver-manager) (2.31.0)\n",
      "Collecting python-dotenv (from webdriver-manager)\n",
      "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from webdriver-manager) (24.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from requests->webdriver-manager) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from requests->webdriver-manager) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from requests->webdriver-manager) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from requests->webdriver-manager) (2023.11.17)\n",
      "Downloading webdriver_manager-4.0.1-py2.py3-none-any.whl (27 kB)\n",
      "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
      "Installing collected packages: python-dotenv, webdriver-manager\n",
      "Successfully installed python-dotenv-1.0.1 webdriver-manager-4.0.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install webdriver-manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests beautifulsoup4 pillow selenium rembg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No suitable image found for Carlos Boozer\n",
      "Saved image for Jordan Hill\n",
      "Saved image for Wesley Johnson\n",
      "Saved image for Kobe Bryant\n",
      "Saved image for Jeremy Lin\n",
      "Saved image for Jordan Clarkson\n",
      "Saved image for Ronnie Price\n",
      "Saved image for Julius Randle\n",
      "Saved image for Xavier Henry\n",
      "Saved image for Ed Davis\n",
      "Saved image for Robert Sacre\n",
      "Saved image for Wayne Ellington\n",
      "Saved image for Ryan Kelly\n",
      "Saved image for Nick Young\n",
      "Saved image for Tarik Black\n",
      "Saved image for Jabari Brown\n",
      "No suitable image found for Dwight Buycks\n",
      "Saved image for Vander Blue\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from io import BytesIO\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.service import Service as ChromeService\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from rembg import remove\n",
    "import time\n",
    "\n",
    "# Function to download and save player images\n",
    "def download_player_image(player_name, save_path):\n",
    "    search_query = f\"Los angeles Lakers {player_name} .png\"\n",
    "    search_url = f\"https://www.google.com/search?tbm=isch&q={search_query}\"\n",
    "    \n",
    "    # Initialize Selenium WebDriver\n",
    "    options = webdriver.ChromeOptions()\n",
    "    options.add_argument('--headless')\n",
    "    driver = webdriver.Chrome(service=ChromeService(ChromeDriverManager().install()), options=options)\n",
    "    driver.get(search_url)\n",
    "    \n",
    "    # Wait for images to load\n",
    "    time.sleep(2)\n",
    "    \n",
    "    # Get image results\n",
    "    image_elements = driver.find_elements(By.CSS_SELECTOR, 'img')\n",
    "    image_url = None\n",
    "    for image_element in image_elements:\n",
    "        src = image_element.get_attribute('src')\n",
    "        alt = image_element.get_attribute('alt')\n",
    "        if src and src.startswith('http') and 'lakers' in alt.lower():\n",
    "            image_url = src\n",
    "            break\n",
    "    \n",
    "    driver.quit()\n",
    "    \n",
    "    if image_url:\n",
    "        image_response = requests.get(image_url)\n",
    "        image = Image.open(BytesIO(image_response.content))\n",
    "\n",
    "        # Remove the background\n",
    "        image = remove(image)\n",
    "\n",
    "        # Convert to PNG and save the image\n",
    "        image = image.convert(\"RGBA\")\n",
    "        image_save_path = os.path.join(save_path, f\"{player_name}.png\")\n",
    "        image.save(image_save_path, format=\"PNG\")\n",
    "        print(f\"Saved image for {player_name}\")\n",
    "    else:\n",
    "        print(f\"No suitable image found for {player_name}\")\n",
    "\n",
    "# Function to create directory if it doesn't exist\n",
    "def create_directory(directory):\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Main function to scrape images for all players in the dataset\n",
    "def main():\n",
    "    # Load player names from the dataset\n",
    "    data_path = 'mergeFinalC_with_local.csv'\n",
    "    shots_data = pd.read_csv(data_path)\n",
    "    players = shots_data['name'].unique()\n",
    "\n",
    "    # Create a directory to save player images\n",
    "    save_path = 'player_images'\n",
    "    create_directory(save_path)\n",
    "\n",
    "    # Download and save images for each player\n",
    "    for player in players:\n",
    "        download_player_image(player, save_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learnNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Downloading scikit_learn-1.5.0-cp311-cp311-win_amd64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: numpy>=1.19.5 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\juani\\onedrive\\documentos\\itba\\.conda\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.5.0-cp311-cp311-win_amd64.whl (11.0 MB)\n",
      "   ---------------------------------------- 0.0/11.0 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.1/11.0 MB 2.0 MB/s eta 0:00:06\n",
      "   - -------------------------------------- 0.4/11.0 MB 4.7 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/11.0 MB 6.6 MB/s eta 0:00:02\n",
      "   ----- ---------------------------------- 1.4/11.0 MB 8.2 MB/s eta 0:00:02\n",
      "   ------- -------------------------------- 2.0/11.0 MB 9.3 MB/s eta 0:00:01\n",
      "   --------- ------------------------------ 2.6/11.0 MB 9.6 MB/s eta 0:00:01\n",
      "   ----------- ---------------------------- 3.3/11.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 3.8/11.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 4.4/11.0 MB 10.9 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 5.1/11.0 MB 11.2 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 5.7/11.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 6.2/11.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/11.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/11.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/11.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/11.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ----------------------- ---------------- 6.4/11.0 MB 11.3 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 7.0/11.0 MB 8.4 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 7.5/11.0 MB 8.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 7.8/11.0 MB 8.7 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 8.0/11.0 MB 8.2 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 9.1/11.0 MB 8.9 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 10.5/11.0 MB 10.4 MB/s eta 0:00:01\n",
      "   ---------------------------------------  11.0/11.0 MB 10.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 11.0/11.0 MB 10.2 MB/s eta 0:00:00\n",
      "Downloading joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "   ---------------------------------------- 0.0/301.8 kB ? eta -:--:--\n",
      "   ---------------------------------------- 301.8/301.8 kB 9.1 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "  Attempting uninstall: joblib\n",
      "    Found existing installation: joblib 1.1.1\n",
      "    Uninstalling joblib-1.1.1:\n",
      "      Successfully uninstalled joblib-1.1.1\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.5.0 threadpoolctl-3.5.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pandas-profiling 3.2.0 requires joblib~=1.1.0, but you have joblib 1.4.2 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RandomForest Accuracy: 0.6168\n",
      "LogisticRegression Accuracy: 0.6102\n",
      "Feature Importance:\n",
      "             feature  importance\n",
      "5                  y    0.164983\n",
      "4                  x    0.152785\n",
      "6  defender_distance    0.142986\n",
      "7         shot_clock    0.136384\n",
      "2  seconds_remaining    0.132641\n",
      "3      shot_distance    0.125795\n",
      "1  minutes_remaining    0.090240\n",
      "0             period    0.054186\n",
      "Feature Importance for 2-pointers:\n",
      "             feature  importance\n",
      "5                  y    0.163193\n",
      "4                  x    0.151216\n",
      "6  defender_distance    0.141536\n",
      "7         shot_clock    0.139342\n",
      "2  seconds_remaining    0.136612\n",
      "3      shot_distance    0.122180\n",
      "1  minutes_remaining    0.091162\n",
      "0             period    0.054759\n",
      "Feature Importance for 3-pointers:\n",
      "             feature  importance\n",
      "4                  x    0.163812\n",
      "5                  y    0.163323\n",
      "6  defender_distance    0.160423\n",
      "7         shot_clock    0.149256\n",
      "2  seconds_remaining    0.146300\n",
      "1  minutes_remaining    0.101813\n",
      "3      shot_distance    0.059092\n",
      "0             period    0.055980\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the dataset\n",
    "data_path = 'mergeFinalC_with_local.csv'\n",
    "data = pd.read_csv(data_path)\n",
    "\n",
    "# Select relevant features for modeling\n",
    "features = ['period', 'minutes_remaining', 'seconds_remaining', 'shot_distance', 'x', 'y', 'defender_distance', 'shot_clock']\n",
    "target = 'shot_made_flag'\n",
    "\n",
    "# Drop rows with missing values in relevant columns\n",
    "data = data.dropna(subset=features + [target])\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X = data[features]\n",
    "y = data[target]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Standardize the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize models\n",
    "models = {\n",
    "    'RandomForest': RandomForestClassifier(random_state=42),\n",
    "    'LogisticRegression': LogisticRegression(random_state=42, max_iter=1000)\n",
    "}\n",
    "\n",
    "# Train and evaluate models\n",
    "model_accuracies = {}\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train_scaled, y_train)\n",
    "    y_pred = model.predict(X_test_scaled)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    model_accuracies[name] = accuracy\n",
    "    print(f\"{name} Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Save model accuracies to a CSV file\n",
    "model_accuracies_df = pd.DataFrame(model_accuracies.items(), columns=['Model', 'Accuracy'])\n",
    "model_accuracies_df.to_csv('model_accuracies.csv', index=False)\n",
    "\n",
    "# Select the best model (assuming RandomForest for this example)\n",
    "best_model = models['RandomForest']\n",
    "\n",
    "# Compute feature importance\n",
    "importances = best_model.feature_importances_\n",
    "feature_importance = pd.DataFrame({'feature': features, 'importance': importances}).sort_values(by='importance', ascending=False)\n",
    "\n",
    "print(\"Feature Importance:\")\n",
    "print(feature_importance)\n",
    "\n",
    "# Plot feature importance\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance)\n",
    "plt.title('Feature Importance')\n",
    "plt.savefig('feature_importance.png')\n",
    "plt.close()\n",
    "\n",
    "# Separate data into 2-pointers and 3-pointers\n",
    "data_2pointers = data[data['shot_distance'] <= 23.75]\n",
    "data_3pointers = data[data['shot_distance'] > 23.75]\n",
    "\n",
    "# Function to train model and get feature importance\n",
    "def analyze_feature_importance(data_subset):\n",
    "    X = data_subset[features]\n",
    "    y = data_subset[target]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    best_model.fit(X_train_scaled, y_train)\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_importance = pd.DataFrame({'feature': features, 'importance': importances}).sort_values(by='importance', ascending=False)\n",
    "    return feature_importance\n",
    "\n",
    "# Analyze feature importance for 2-pointers\n",
    "feature_importance_2pointers = analyze_feature_importance(data_2pointers)\n",
    "print(\"Feature Importance for 2-pointers:\")\n",
    "print(feature_importance_2pointers)\n",
    "\n",
    "# Save feature importance for 2-pointers to a CSV file\n",
    "feature_importance_2pointers.to_csv('feature_importance_2pointers.csv', index=False)\n",
    "\n",
    "# Analyze feature importance for 3-pointers\n",
    "feature_importance_3pointers = analyze_feature_importance(data_3pointers)\n",
    "print(\"Feature Importance for 3-pointers:\")\n",
    "print(feature_importance_3pointers)\n",
    "\n",
    "# Save feature importance for 3-pointers to a CSV file\n",
    "feature_importance_3pointers.to_csv('feature_importance_3pointers.csv', index=False)\n",
    "\n",
    "# Plot feature importance for 2-pointers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_2pointers)\n",
    "plt.title('Feature Importance for 2-pointers')\n",
    "plt.savefig('feature_importance_2pointers.png')\n",
    "plt.close()\n",
    "\n",
    "# Plot feature importance for 3-pointers\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.barplot(x='importance', y='feature', data=feature_importance_3pointers)\n",
    "plt.title('Feature Importance for 3-pointers')\n",
    "plt.savefig('feature_importance_3pointers.png')\n",
    "plt.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
